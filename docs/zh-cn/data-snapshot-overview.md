# 股票数据存储方案说明

> 文档版本：v1.0 | 日期：2026-02-25

## 一、背景与动机

在使用 AI Hedge Fund 系统分析股票时，Tushare 等数据源获取的股票数据（价格、财务指标、资产负债表、现金流量表等）目前仅在内存中短暂存在，经过三级缓存（LRU → Redis → SQLite pickle）后即被丢弃。这带来了三个问题：

- **不可回溯**：无法查看某次分析时使用的原始数据，难以复盘 AI 的决策依据
- **不可阅读**：缓存使用 pickle 二进制格式，人类无法直接打开查看
- **不可复用**：相同数据每次运行都需重新获取，浪费 API 调用额度

因此，需要在每次分析时，将获取的股票数据以**人类可读、AI 可用**的格式保存为文件。

## 二、当前数据流分析

### 数据来源

系统通过 `src/tools/api.py` 统一网关获取数据，对于 A 股自动路由到 Tushare：

| 数据类型 | Tushare 接口 | 返回模型 | 典型数据量 |
|----------|-------------|----------|-----------|
| 价格（OHLCV） | `pro.daily` | `Price` | 250条/年，约15KB |
| 财务指标 | `pro.fina_indicator` | `FinancialMetrics` | 4-10期，约5KB/期 |
| 资产负债表 | `pro.balancesheet` | `LineItem` | 4-10期，约3KB/期 |
| 现金流量表 | `pro.cashflow` | `LineItem` | 4-10期，约3KB/期 |
| 利润表 | `pro.income` | `LineItem` | 4-10期，约3KB/期 |
| 市值 | `pro.daily_basic` | `float` | 单值，约100B |

单次分析每个 ticker 的总数据量约 **50-80KB**，属于小数据量场景。

### 数据模型

所有数据统一为 Pydantic 模型（定义在 `src/data/models.py`）：

- `Price`：开盘价、最高价、最低价、收盘价、成交量、日期
- `FinancialMetrics`：40+ 字段，涵盖估值、盈利能力、流动性、杠杆、成长性
- `LineItem`：灵活的财务科目模型，支持动态字段（`model_config = {"extra": "allow"}`）

### 现有缓存机制

三级缓存架构（`src/data/enhanced_cache.py`）：

| 层级 | 存储 | 格式 | 人可读 | AI可用 |
|------|------|------|--------|--------|
| L1 | 内存 LRU（128条） | Python对象 | ❌ | ❌ |
| L2 | Redis（可选） | pickle 二进制 | ❌ | ❌ |
| L3 | SQLite 磁盘 | pickle BLOB | ❌ | ❌ |

**结论**：现有缓存解决了性能问题，但完全不满足"人能看、AI能用"的需求。

## 三、格式对比评估

针对本项目的小数据量、教育研究场景，评估了六种候选格式：

| 维度 | Markdown | Parquet | DuckDB | JSON | CSV | SQLite |
|------|----------|---------|--------|------|-----|--------|
| 人直接打开看 | ⭐⭐⭐⭐⭐ | ❌ 二进制 | ❌ 要工具 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ 要工具 |
| 喂给 LLM | ⭐⭐⭐⭐⭐ | ❌ 要转换 | ❌ 要查询 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ 要查询 |
| Token 效率 | 最优 | — | — | 比MD多50-200% | 接近MD | — |
| 程序化查询 | ⭐ 差 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐ | ⭐⭐⭐⭐ |
| 存储效率 | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐ |
| Git 友好度 | ⭐⭐⭐⭐⭐ | ❌ | ❌ | ⭐⭐⭐ | ⭐⭐⭐ | ❌ |
| 新增依赖 | 无 | pyarrow | duckdb | 无 | 无 | 无 |

### 开源量化项目的选择参考

| 项目 | 存储方案 | 适用场景 |
|------|----------|----------|
| Zipline（Quantopian） | Bcolz 列式压缩 + SQLite | 百万级分钟线数据 |
| Qlib（Microsoft） | 自定义二进制 .bin | 大规模因子计算 |
| Backtrader | 纯内存 DataFrame | 小规模回测 |

这些项目选择重格式是因为它们处理**百万级数据点**。本项目每次分析仅 50-80KB，性能不是瓶颈。

## 四、推荐方案：Markdown + JSON 双格式

### 选择理由

1. **Markdown 为主**：解决"人能看 + AI能用"的核心需求。Markdown 表格的 Token 效率最高，LLM 理解准确率最好，且任何编辑器都能直接打开
2. **JSON 为辅**：解决"程序能查"的补充需求。当需要用脚本批量分析历史数据时，JSON 提供结构化访问能力
3. **零新依赖**：纯 Python 标准库即可实现，不需要引入 pyarrow、duckdb 等额外包
4. **Git 友好**：两种格式都是纯文本，commit 后可以清晰看到数据变化

### 不选其他方案的理由

- **Parquet/DuckDB**：适合大数据量分析场景，对本项目属于过度工程。引入额外依赖，人无法直接阅读
- **CSV**：缺少层次结构，无法优雅地在一个文件中组织多类数据
- **纯 JSON**：Token 消耗比 Markdown 高 50-200%，人类阅读体验差
- **SQLite**：已经用于缓存层，数据快照不需要关系查询能力

### 文件组织

按 `ticker / 分析日期` 组织快照目录，每次分析生成一组文件：

- `summary.md`：人和 AI 的主要阅读入口，包含格式化的表格和摘要
- `prices.json`：完整价格数据的结构化备份
- `financials.json`：完整财务数据的结构化备份
- `index.json`：全局索引，记录有哪些 ticker 和日期的快照

### 预期效果

- 打开 `summary.md` 即可看到某只股票在某个时间点的完整数据快照
- 将 `summary.md` 内容直接粘贴给 LLM，无需任何转换
- 用脚本遍历 JSON 文件，即可批量分析历史数据
- Git diff 可以清晰看到两次分析之间数据的变化

## 五、下一步

详细的技术设计方案请参见 [数据快照系统详细设计](./data-snapshot-design.md)。
